"""
Crawler Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ Ø§ÛŒØ±Ø§Ù† (rrk.ir)
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Threading - Ù‡Ø± Ø±ÙˆØ² Ø¯Ø± ÛŒÚ© Thread Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡

Ù†Ø³Ø®Ù‡ Multi-Threading Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import jdatetime
from datetime import datetime
import pandas as pd
import json
import time
import logging
import os
from threading import Thread, Lock
from queue import Queue
import concurrent.futures

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(threadName)-10s] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/rrk_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class RRKCrawler:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Crawler Ø¨Ø§ Selenium Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Threading"""
    
    def __init__(self, headless=True, thread_name=None):
        self.base_url = "https://rrk.ir"
        self.search_url = "https://rrk.ir/ords/r/rrs/rrs-front/Ø¯Ø§Ø¯Ù‡-Ø¨Ø§Ø²"
        self.all_ads = []
        self.headless = headless
        self.driver = None
        self.counter = 0
        self.thread_name = thread_name or "MainThread"
        self.lock = Lock()  # Ø¨Ø±Ø§ÛŒ thread-safe operations
    
    def init_driver(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Selenium WebDriver Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Headless"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--start-maximized')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument('--disable-extensions')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--lang=fa-IR')
        chrome_options.add_argument('--accept-lang=fa-IR,fa')
        
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_experimental_option("prefs", {
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_settings.popups": 0,
        })
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            if self.headless:
                self.driver.set_window_size(1920, 1080)
            else:
                self.driver.maximize_window()
            
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info(f"âœ“ WebDriver Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯ (Thread: {self.thread_name})")
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ WebDriver Ø¯Ø± {self.thread_name}: {e}")
            raise
    
    def wait_for_page_load(self, timeout=15):
        """Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡"""
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda d: d.execute_script('return document.readyState') == 'complete'
            )
            
            try:
                WebDriverWait(self.driver, 5).until(
                    lambda d: d.execute_script('return typeof jQuery != "undefined" && jQuery.active == 0')
                )
            except:
                pass
            
            sleep_time = 2 if self.headless else 1
            time.sleep(sleep_time)
            
        except TimeoutException:
            logging.warning(f"ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ ({self.thread_name})")
    
    def fill_search_form(self, date_from, date_to=None):
        """Ù¾Ø± Ú©Ø±Ø¯Ù† ÙØ±Ù… Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        if not date_to:
            date_to = date_from
        
        try:
            logging.info(f"ğŸ” Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ ({self.thread_name})...")
            self.driver.get(self.search_url)
            self.wait_for_page_load(timeout=20)
            
            os.makedirs('screenshots', exist_ok=True)
            self.driver.save_screenshot(f'screenshots/step1_{self.thread_name}.png')
            
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(1)
            
            try:
                wait = WebDriverWait(self.driver, 20)
                
                date_from_field = wait.until(
                    EC.presence_of_element_located((By.ID, "P199_NEWSPAPERDATE_AZ"))
                )
                
                self.driver.execute_script("arguments[0].scrollIntoView(true);", date_from_field)
                time.sleep(0.5)
                
                date_from_field = wait.until(
                    EC.element_to_be_clickable((By.ID, "P199_NEWSPAPERDATE_AZ"))
                )
                
                date_to_field = self.driver.find_element(By.ID, "P199_NEWSPAPER_TA")
                
                self.driver.execute_script(f"arguments[0].value = '{date_from}';", date_from_field)
                self.driver.execute_script("arguments[0].dispatchEvent(new Event('change', { bubbles: true }));", date_from_field)
                logging.info(f"âœ“ ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹: {date_from} ({self.thread_name})")
                
                self.driver.execute_script(f"arguments[0].value = '{date_to}';", date_to_field)
                self.driver.execute_script("arguments[0].dispatchEvent(new Event('change', { bubbles: true }));", date_to_field)
                logging.info(f"âœ“ ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†: {date_to} ({self.thread_name})")
                
                time.sleep(1)
                self.driver.save_screenshot(f'screenshots/step2_{self.thread_name}.png')
                
                search_button = wait.until(
                    EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Ø¬Ø³ØªØ¬Ùˆ') or contains(@id, 'search') or contains(@class, 'search')]"))
                )
                
                self.driver.execute_script("arguments[0].scrollIntoView(true);", search_button)
                time.sleep(0.5)
                
                self.driver.execute_script("arguments[0].click();", search_button)
                logging.info(f"âœ“ Ø¯Ú©Ù…Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù„ÛŒÚ© Ø´Ø¯ ({self.thread_name})")
                
                self.wait_for_page_load(timeout=20)
                self.driver.save_screenshot(f'screenshots/step3_{self.thread_name}.png')
                
                return True
                
            except (NoSuchElementException, TimeoutException) as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø± Ú©Ø±Ø¯Ù† ÙØ±Ù… ({self.thread_name}): {e}")
                return False
                
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± ÙØ±Ù… ({self.thread_name}): {e}")
            return False
    
    def extract_ads_from_page(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ØµÙØ­Ù‡ ÙØ¹Ù„ÛŒ"""
        ads = []
        
        try:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            os.makedirs('data', exist_ok=True)
            with open(f'data/page_{self.thread_name}_{self.counter}.html', 'w', encoding='utf-8') as f:
                f.write(soup.prettify())
            self.counter += 1
            
            tables = soup.find_all('table')
            logging.info(f"ğŸ“Š {len(tables)} Ø¬Ø¯ÙˆÙ„ ({self.thread_name})")
            
            for table in tables:
                rows = table.find_all('tr')
                
                for i, row in enumerate(rows):
                    if i == 0:
                        continue
                    
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        ad_data = {
                            'Ø±Ø¯ÛŒÙ': i,
                            'Ø³ØªÙˆÙ†_1': cells[0].get_text(strip=True) if len(cells) > 0 else '',
                            'Ø³ØªÙˆÙ†_2': cells[1].get_text(strip=True) if len(cells) > 1 else '',
                            'Ø³ØªÙˆÙ†_3': cells[2].get_text(strip=True) if len(cells) > 2 else '',
                            'Ø³ØªÙˆÙ†_4': cells[3].get_text(strip=True) if len(cells) > 3 else '',
                            'Ø³ØªÙˆÙ†_5': cells[4].get_text(strip=True) if len(cells) > 4 else '',
                            'Ø³ØªÙˆÙ†_6': cells[5].get_text(strip=True) if len(cells) > 5 else '',
                        }
                        
                        link = row.find('a')
                        if link and link.get('href'):
                            ad_data['Ù„ÛŒÙ†Ú©'] = link['href']
                        
                        ads.append(ad_data)
            
            result_divs = soup.find_all('div', class_=lambda x: x and any(
                keyword in str(x).lower() for keyword in ['result', 'item', 'card', 'row', 'ad']
            ))
            
            if not ads and result_divs:
                for div in result_divs[:10]:
                    text = div.get_text(strip=True)
                    if len(text) > 20:
                        ads.append({
                            'Ù…Ø­ØªÙˆØ§': text[:200],
                            'html': str(div)[:500]
                        })
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ({self.thread_name}): {e}")
        
        logging.info(f"âœ“ {len(ads)} Ø¢Ú¯Ù‡ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ ({self.thread_name})")
        return ads
    
    def check_next_page(self):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ"""
        try:
            wait = WebDriverWait(self.driver, 10)
            next_button = wait.until(
                EC.presence_of_element_located((By.XPATH, 
                    "//button[contains(text(), 'Ø¨Ø¹Ø¯ÛŒ')] | //a[contains(text(), 'Ø¨Ø¹Ø¯ÛŒ')] | //button[contains(@class, 'next')]"
                ))
            )
            
            if next_button.is_enabled() and next_button.is_displayed():
                self.driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
                time.sleep(0.5)
                
                self.driver.execute_script("arguments[0].click();", next_button)
                self.wait_for_page_load()
                return True
        except (NoSuchElementException, TimeoutException):
            logging.info(f"ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ({self.thread_name})")
        
        return False
    
    def search_by_date(self, date):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© ØªØ§Ø±ÛŒØ® Ø®Ø§Øµ"""
        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®: {date} ({self.thread_name})")
        logging.info(f"{'='*60}")
        
        all_date_ads = []
        
        if self.fill_search_form(date):
            page_num = 1
            
            while True:
                if page_num % 10 == 1:
                    time.sleep(6)
                
                logging.info(f"ğŸ“„ ØµÙØ­Ù‡ {page_num} ({self.thread_name})...")
                ads = self.extract_ads_from_page()
                if ads:
                    for ad in ads:
                        ad['ØªØ§Ø±ÛŒØ®_Ø¬Ø³ØªØ¬Ùˆ'] = date
                        ad['Ø´Ù…Ø§Ø±Ù‡_ØµÙØ­Ù‡'] = page_num
                        ad['thread'] = self.thread_name
                    all_date_ads.extend(ads)
                
                if not self.check_next_page():
                    break
                
                page_num += 1
                time.sleep(0.1)
        
        logging.info(f"âœ“ {len(all_date_ads)} Ø¢Ú¯Ù‡ÛŒ Ø¨Ø±Ø§ÛŒ {date} ({self.thread_name})")
        return all_date_ads
    
    def crawl_date(self, date):
        """Crawl ÛŒÚ© ØªØ§Ø±ÛŒØ® Ø®Ø§Øµ - Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Thread"""
        try:
            self.init_driver()
            ads = self.search_by_date(date)
            return ads
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± crawl ØªØ§Ø±ÛŒØ® {date} ({self.thread_name}): {e}")
            return []
        finally:
            if self.driver:
                self.driver.quit()
                logging.info(f"âœ“ WebDriver Ø¨Ø³ØªÙ‡ Ø´Ø¯ ({self.thread_name})")


class ThreadedRRKCrawler:
    """Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Crawler Ø¨Ø§ Threading"""
    
    def __init__(self, headless=True, max_workers=5):
        self.headless = headless
        self.max_workers = max_workers
        self.all_results = []
        self.lock = Lock()
    
    def get_last_n_days(self, n=10):
        """Ø¯Ø±ÛŒØ§ÙØª n Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡"""
        dates = []
        today = jdatetime.date.today()
        
        for i in range(1, n + 1):
            date = today - jdatetime.timedelta(days=i)
            dates.append(date.strftime('%Y/%m/%d'))
        
        logging.info(f"ğŸ“… ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù ({len(dates)} Ø±ÙˆØ²): {dates}")
        return dates
    
    def crawl_date_worker(self, date, thread_id):
        """Worker function Ø¨Ø±Ø§ÛŒ Ù‡Ø± thread"""
        thread_name = f"Thread-{thread_id}"
        logging.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ Thread {thread_id} Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {date}")
        
        crawler = RRKCrawler(headless=self.headless, thread_name=thread_name)
        ads = crawler.crawl_date(date)
        
        # Thread-safe Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬
        with self.lock:
            self.all_results.extend(ads)
        
        logging.info(f"âœ… Thread {thread_id} ØªÙ…Ø§Ù… Ø´Ø¯ - {len(ads)} Ø¢Ú¯Ù‡ÛŒ")
        return len(ads)
    
    def crawl_all_parallel(self, num_days=10):
        """Crawl Ù…ÙˆØ§Ø²ÛŒ ØªÙ…Ø§Ù… Ø±ÙˆØ²Ù‡Ø§"""
        start_time = time.time()
        
        print("\n" + "="*70)
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Crawling Ù…ÙˆØ§Ø²ÛŒ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ ThreadÙ‡Ø§: {self.max_workers}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§: {num_days}")
        print("="*70 + "\n")
        
        dates = self.get_last_n_days(num_days)
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ThreadPoolExecutor Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ±
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Ø§Ø±Ø³Ø§Ù„ ØªÙ…Ø§Ù… taskÙ‡Ø§
            future_to_date = {
                executor.submit(self.crawl_date_worker, date, i): date 
                for i, date in enumerate(dates, 1)
            }
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†ØªØ§ÛŒØ¬
            completed = 0
            for future in concurrent.futures.as_completed(future_to_date):
                date = future_to_date[future]
                try:
                    num_ads = future.result()
                    completed += 1
                    print(f"âœ“ [{completed}/{len(dates)}] ØªØ§Ø±ÛŒØ® {date} ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯ - {num_ads} Ø¢Ú¯Ù‡ÛŒ")
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {date}: {e}")
        
        end_time = time.time()
        duration = end_time - start_time
        
        print("\n" + "="*70)
        print("âœ… Crawling Ù…ÙˆØ§Ø²ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
        print(f"   ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§: {len(self.all_results)}")
        print(f"   â±ï¸  Ø²Ù…Ø§Ù† Ú©Ù„: {duration:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"   âš¡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {duration/len(dates):.2f} Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ø±ÙˆØ²")
        print("="*70 + "\n")
        
        df = pd.DataFrame(self.all_results) if self.all_results else pd.DataFrame()
        return df
    
    def save_results(self, df, formats=['csv', 'json', 'excel']):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬"""
        if df.empty:
            logging.warning("âš ï¸  Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
            return
        
        os.makedirs('output', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if 'csv' in formats:
            csv_file = f'output/rrk_ads_{timestamp}.csv'
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… CSV: {csv_file}")
        
        if 'json' in formats:
            json_file = f'output/rrk_ads_{timestamp}.json'
            df.to_json(json_file, orient='records', force_ascii=False, indent=2)
            print(f"âœ… JSON: {json_file}")
        
        if 'excel' in formats:
            try:
                excel_file = f'output/rrk_ads_{timestamp}.xlsx'
                df.to_excel(excel_file, index=False, engine='openpyxl')
                print(f"âœ… Excel: {excel_file}")
            except Exception as e:
                logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Excel: {e}")
    
    def generate_report(self, df):
        """Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„"""
        if df.empty:
            return
        
        print(f"\nğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¢Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„:")
        print(f"{'='*60}")
        print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§: {len(df)}")
        print(f"   â€¢ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {list(df.columns)}")
        
        if 'ØªØ§Ø±ÛŒØ®_Ø¬Ø³ØªØ¬Ùˆ' in df.columns:
            print(f"   â€¢ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø´Ø¯Ù‡: {df['ØªØ§Ø±ÛŒØ®_Ø¬Ø³ØªØ¬Ùˆ'].nunique()}")
            print(f"\n   ğŸ“… ØªÙˆØ²ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®:")
            for date, count in df['ØªØ§Ø±ÛŒØ®_Ø¬Ø³ØªØ¬Ùˆ'].value_counts().sort_index().items():
                print(f"      - {date}: {count:,} Ø¢Ú¯Ù‡ÛŒ")
        
        if 'thread' in df.columns:
            print(f"\n   ğŸ§µ ØªÙˆØ²ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Thread:")
            for thread, count in df['thread'].value_counts().items():
                print(f"      - {thread}: {count:,} Ø¢Ú¯Ù‡ÛŒ")
        
        print(f"{'='*60}\n")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    start_time = time.time()
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
    for folder in ['logs', 'data', 'screenshots', 'output']:
        os.makedirs(folder, exist_ok=True)
    
    print("\n" + "="*70)
    print("ğŸ“° Crawler Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ø±Ø³Ù…ÛŒ Ø§ÛŒØ±Ø§Ù† (rrk.ir)")
    print("   ğŸ§µ Ù†Ø³Ø®Ù‡ Multi-Threading - Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ")
    print("="*70 + "\n")
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    NUM_DAYS = 10  # ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡
    MAX_WORKERS = 5  # ØªØ¹Ø¯Ø§Ø¯ Thread Ù‡Ù…Ø²Ù…Ø§Ù† (ØªÙˆØµÛŒÙ‡: 3-5 Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†)
    HEADLESS = True  # Ø­Ø§Ù„Øª headless
    
    print(f"âš™ï¸  ØªÙ†Ø¸ÛŒÙ…Ø§Øª:")
    print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§: {NUM_DAYS}")
    print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Thread Ù‡Ù…Ø²Ù…Ø§Ù†: {MAX_WORKERS}")
    print(f"   â€¢ Ø­Ø§Ù„Øª Headless: {HEADLESS}")
    print()
    
    # Ø§Ø¬Ø±Ø§
    crawler = ThreadedRRKCrawler(headless=HEADLESS, max_workers=MAX_WORKERS)
    
    try:
        df = crawler.crawl_all_parallel(num_days=NUM_DAYS)
        
        if not df.empty:
            crawler.generate_report(df)
            crawler.save_results(df)
            
            print(f"\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ (10 Ø±Ú©ÙˆØ±Ø¯ Ø§ÙˆÙ„):")
            print(df.head(10).to_string())
        else:
            print("\nâš ï¸  Ù‡ÛŒÚ† Ø¢Ú¯Ù‡ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            print("\nğŸ’¡ Ù†Ú©Ø§Øª Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ:")
            print("   1. ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª Ø¯Ø± Ù¾ÙˆØ´Ù‡ screenshots Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
            print("   2. ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ HTML Ø¯Ø± Ù¾ÙˆØ´Ù‡ data Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯")
            print("   3. Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ logs Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
        logging.info("Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}", exc_info=True)
        print(f"\nâŒ Ø®Ø·Ø§: {e}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print("\n" + "="*70)
    print("âœ… Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡")
    print(f"â±ï¸  Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§: {total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡ ({total_time/60:.2f} Ø¯Ù‚ÛŒÙ‚Ù‡)")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()